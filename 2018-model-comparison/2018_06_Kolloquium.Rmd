---
title: "Performance evaluation and hyperparameter sensitivity analysis of parametric and machine-learning models using spatial data"
subtitle: "<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>"
author: "Patrick Schratz"
date: "GIScience Seminar Series, Jena, 17 Jan 2018"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      beforeInit: ["macros.js", "https://platform.twitter.com/widgets.js"]
      ratio: "4:3"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    seal: false
    #chakra: "libs/remark-latest.min.js"
    css: ["mtheme.css", "font-mtheme.css"]
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
htmltools::tagList(rmarkdown::html_dependency_ionicons())
icon_system_file <- function(file) {
  system.file(file, package = "icon")
}
htmltools::htmlDependency("academicons", "1.8.0", src = icon_system_file("fonts/academicons-1.8.0"),
      stylesheet = "css/academicons.min.css")
pacman::p_load(here, knitr, magrittr)
```

```{r, load_refs, echo=FALSE, cache=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           cite.style = 'authoryear', 
           style = "markdown",
           hyperlink = FALSE, 
           dashed = TRUE)
bib <- ReadBib("/home/patrick/PhD/papers/01_model_comparison/02_writing/submission/2/latex-source-files/Bibliography.bib", 
                 check = FALSE)
```

class: title-slide  

# Performance evaluation and hyperparameter tuning of statistical and machine-learning models using spatial data

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

### Patrick Schratz<sup>1</sup>, Jannes Muenchow<sup>1</sup>, Jakob Richter<sup>2</sup>, Alexander Brenning<sup>1</sup>

<p style="margin-left:15px;">

<br>

Kolloquium (Department of Statistics), LMU Munich, 20 Jun 2018

<br><br>

<i class="fa fa-university"></i> <sup>1</sup> Department of Geography, GIScience group, University of Jena  <a href="http://www.geographie.uni-jena.de/en/Geoinformatik_p_1558.html"><i class="fa fa-external-link"></i></a> <br>

<i class="fa fa-university"></i> <sup>2</sup> Department of Statistics, TU Dortmund
<a href="https://www.statistik.tu-dortmund.de/aktuelles.html"><i class="fa fa-external-link"></i></a>
<br><br>

<i class="fa fa-home"></i> <a href="https://pjs-web.de">https://pjs-web.de</a> &emsp; 

<i class="fa fa-twitter"></i> <a href="https://twitter.com/pjs_228">@pjs_228</a> &emsp; 

<i class="fa fa-github"></i> <a href="https://github.com/pat-s">@pat-s</a> &emsp; 

<i class="fa fa-stack-exchange"></i> <a href="https://stackoverflow.com/users/4185785/pat-s">@pjs_228</a> &emsp;  <br>

<i class="fa fa-envelope"></i> <a href="patrick.schratz@uni-jena.de">patrick.schratz@uni-jena.de</a>&emsp;

<i class="fa fa-linkedin"></i> <a href="https://www.linkedin.com/in/patrick-schratz/">Patrick Schratz</a>&emsp;

</p>

<div class="my-header"><img src="figs/life.jpg" style="width = 5%;" /></div> 

---

layout: true

<div class="my-header"><img src="figs/life.jpg" style="width = 5%;" /></div>       

---


# Outline

.pull-left[
.font150[
1. Introduction

2. Data and study area

3. Methods

4. Results

5. Discussion
]]

.pull-right[
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Slides of my upcoming talk at LMU Munich on Jun 20th: <a href="https://t.co/SyWRky6sGn">https://t.co/SyWRky6sGn</a></p>&mdash; Patrick Schratz (@pjs_228) <a href="https://twitter.com/pjs_228/status/1008803282029088774?ref_src=twsrc%5Etfw">June 18, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
]

---
class: inverse, center, middle

# Introduction

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=720px></html> 

---

# Introduction

#### \# Whoami

- "Data Scientist/Analyst"
- B.Sc. **Geography** & M.Sc. **Geoinformatics** at University of Jena
- Self-taught programmer
- Interested in model optimization, R package development, server administration
- Arch Linux package maintainer
- PhD student (since 2016)

#### Contributions to `mlr`
- Integrated new sampling scheme for CV: Spatial sampling
- Redesigned the tutorial site (`mkdocs`-> `pkgdown`)
- Added getter for inner resampling indices
- more to come ;)

---

# Introduction

.pull-left[

### LIFE Healthy Forest <i class="fa fa-tree"></i>
 
Early detection and advanced management systems to reduce forest decline by invasive and pathogenic agents.

**Main task**: Spatial (modeling) analysis to support the early detection of various pathogens.

## Pathogens <i class="fa fa-bug"></i>

* Fusarium circinatum 
* **Diplodia sapinea** (<i class="fa fa-arrow-right"></i> needle blight)
* Armillaria root disease
* Heterobasidion annosum

]

.pull-right[

.center[
![:scale 90%](figs/diplodia.jpg)  
.font70[**Fig. 1:** Needle blight caused by **Diplodia pinea**]
]
]

---

# Introduction

## Motivation

* Find the model with the **highest predictive performance**.

* Results are assumed to be representative for data sets with similar predictors and different pathogens (response).

* Be aware of **spatial autocorrelation** <i class="fa fa-exclamation-triangle"></i>

* Analyze differences between spatial and non-spatial hyperparameter tuning (no research here yet!).

* Analyze differences in performance between algorithms and sampling schemes in CV (both performance estimation and hyperparameter tuning)

---
class: inverse, center, middle

# Data <i class="fa fa-database"></i> & Study Area <i class="fa fa-map"></i>

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=720px></html>

---

# Data <i class="fa fa-database"></i> & Study Area <i class="fa fa-map"></i>

.code70[
```{r, echo = FALSE, results='markup'}
readRDS("/data/patrick/mod/survey_data/2009-2012/data-clean.rda") %>% 
  tibble::as_tibble() -> df
df$geometry <- NULL
df$id <- NULL
df$fus01 <- NULL
df$hail <- NULL 
df$x <- NULL
df$y <- NULL
skimr::skim_format(.levels = list(max_char = 4))
skimr::skim_with(numeric = list(sd = NULL, p25 = NULL, p75 = NULL, complete = NULL), 
                 factor = list(ordered = NULL, complete = NULL))
skimr::skim(df) %>% skimr::kable()
```
]

---

# Data <i class="fa fa-database"></i> & Study Area <i class="fa fa-map"></i>

.center[
![:scale 100%](figs/study_area.png)  
.font70[**Fig. 2:** Study area (Basque Country, Spain)]
]

---
class: inverse, center, middle

# Methods <i class="fa fa-cogs"></i>

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=720px></html> 

---

# Methods <i class="fa fa-cogs"></i>

## Machine-learning models

* Boosted Regression Trees (`BRT`)
* Random Forest (`RF`)
* Support Vector Machine (`SVM`)
* k-nearest Neighbor (`KNN`)

## Parametric models

* Generalized Addtitive Model (`GAM`)
* Generalized Linear Model (`GLM`)

## Performance Measure

Brier Score

---

# Methods <i class="fa fa-cogs"></i>

## Nested Cross-Validation

 * Cross-validation for **performance estimation** 

 * Cross-validation for **hyperparameter tuning** (sequential model based optimization, `r Citet(bib, "mlrmbo")`)
    
Different sampling strategies (Performance estimation/Tuning):

* Non-Spatial/Non-Spatial

* Spatial/Non-Spatial

* Spatial/Spatial

* Non-Spatial/No Tuning

* Spatial/No Tuning

---

# Methods <i class="fa fa-cogs"></i>

## Nested (spatial) cross-validation

.center[
![:scale 100%](figs/cross-validation_farbig.png)  
.font70[**Fig. 3:** Nested spatial/non-spatial cross-validation]] 

---

# Methods <i class="fa fa-cogs"></i>

## Nested (spatial) cross-validation

<br>

.center[
![:scale 100%](figs/spcv_nspcv_folds_pres.png)  
.font70[**Fig. 4:** Comparison of spatial and non-spatial partitioning of the data set.]
]

---

# Methods <i class="fa fa-cogs"></i>

#### Hyperparameter tuning search spaces

RF : `r Citet(bib, "Probst2018b")`  
BRT, SVM, KNN: Self-defined limits based on evaluation of estimated hyperparameters

.center[
![](figs/tuning_limits.png)  
.font70[**Table 1:** Hyperparameter limits and types of each model.  
Notations of hyperparameters from the respective R packages were used.  
$p$ = Number of variables.]
]

---
class: inverse, center, middle

# Results <i class="fa fa-image"></i> 

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=720px></html> 

---

# Results <i class="fa fa-image"></i> 

## Hyperparameter tuning

.center[ 
![:scale 70%](figs/opt-paths-RF-sp-vs-nsp.png)  
]
.font70[**Fig 4:** SMBO optimization paths of the first five folds of the **spatial/spatial** and **spatial/non-spatial** CV setting for RF. The dashed line marks the border between the initial design (30 randomly composed hyperparameter settings) and the sequential optimization part in which each setting was proposed using information from the prior evaluated settings.
]

---

# Results <i class="fa fa-image"></i> 

## Hyperparameter tuning

.center[ 
![:scale 67%](figs/tuning_effects_all_models_mbo.png) 
]
.font70[**Fig 5:** Best hyperparameter settings by fold (500 total) each estimated from 100 (30/70) SMBO tuning iterations per fold using five-fold cross-validation. Split by spatial and non-spatial partitioning setup and model type. 
Red crosses indicate the default hyperparameters of the respective model.
Black dots represent the winning hyperparameter setting of each fold.
The labels ranging from one to five show the winning hyperparameter settings of the first five folds.
]

---

# Results <i class="fa fa-image"></i>

## Predictive Performance

.center[
![:scale 55%](figs/cv_boxplots_final_brier.png)  
]
.font70[**Fig 6:**  (Nested) CV estimates of model performance at the repetition level using 100 SMBO iterations for hyperparameter tuning.
CV setting refers to performance estimation/hyperparameter tuning of the respective (nested) CV, e.g. "Spatial/Non-Spatial" means that spatial partitioning was used for performance estimation and non-spatial partitioning for hyperparameter tuning.
]

---
class: inverse, center, middle

# Discussion <i class="fa fa-comments"></i>

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=720px></html> 

---

# Discussion <i class="fa fa-comments"></i>

## Predictive performance

* `RF` showed the best predictive performance <i class="fa fa-trophy"></i>

--

* High bias in performance when using non-spatial CV

---

# Discussion <i class="fa fa-comments"></i> (Performance)

.center[
![:scale 55%](figs/cv_boxplots_final_brier.png)  
]
.font70[**Fig 6:**  (Nested) CV estimates of model performance at the repetition level using 100 SMBO iterations for hyperparameter tuning.
CV setting refers to performance estimation/hyperparameter tuning of the respective (nested) CV, e.g. "Spatial/Non-Spatial" means that spatial partitioning was used for performance estimation and non-spatial partitioning for hyperparameter tuning.
]

---

# Discussion <i class="fa fa-comments"></i>

## Predictive Performance

* `RF` showed the best predictive performance <i class="fa fa-trophy"></i>

* High bias in performance when using non-spatial CV

--

* The `GLM` shows an equally good performance as BRT, KNN and SVM

--

* The `GAM` suffers from overfitting

---

# Discussion <i class="fa fa-comments"></i>

## Hyperparameter tuning

* Almost no effect on predictive performance.

--

* Differences between algorithms are higher than the effect of hyperparameter tuning

--

* Spatial hyperparameter tuning has no substantial effect on predictive performance compared to non-spatial tuning

--

* Optimal parameters estimated from spatial hyperparameter tuning show a wide spread across the search space

---

# Discussion <i class="fa fa-comments"></i> 

## Tuning

.center[ 
![:scale 70%](figs/tuning_effects_all_models_mbo.png)  
]
.font70[**Fig 5:** Best hyperparameter settings by fold (500 total) each estimated from 100 (30/70) SMBO tuning iterations per fold using five-fold cross-validation. Split by spatial and non-spatial partitioning setup and model type. 
Red crosses indicate the default hyperparameters of the respective model.
Black dots represent the winning hyperparameter setting of each fold.
The labels ranging from one to five show the winning hyperparameter settings of the first five folds.
]

---

# Discussion <i class="fa fa-comments"></i>

## Hyperparameter tuning

* Almost no effect on predictive performance.

* Differences between algorithms are higher than the effect of hyperparameter tuning.

* Spatial hyperparameter tuning has no substantial effect on predictive performance compared to non-spatial tuning.

* Optimal parameters estimated from spatial hyperparameter tuning show a wide spread across the search space.

<i class="fa fa-exclamation-circle"></i> Spatial hyperparameter tuning should be used for spatial data sets to have a consistent resampling scheme. <i class="fa fa-exclamation-circle"></i>

---

# References <i class="fa fa-copy"></i>

```{r, results='asis', echo=FALSE} 
PrintBibliography(bib)
```

<br>

.center[
## Thanks for listening!

Questions? Slides can be found here: https://t.co/SyWRky6sGn

And now, let's have a <i class="fa fa-beer"></i> ;)
]

---
class: inverse, center, middle

# Backup <i class="fa fa-plus-square"></i>

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=720px></html> 

---

# Backup <i class="fa fa-plus-square"></i>

.center[
![:scale 85%](figs/tuning_effects_appendix_mbo.png)
] 
