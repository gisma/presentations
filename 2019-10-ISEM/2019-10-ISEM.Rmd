---
title: "Presentation of the new High-Performance-Cluster"
subtitle: "<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>"
author: "Patrick Schratz"
date: "ISEM 2019, October 4th 2019, Salzburg, Austria"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      ratio: "16:9"
      highlightStyle: solarized-light
      highlightLines: true
      countIncrementalSlides: false
    seal: false
    beforeInit: assets/js/zoom.js
    css: 
       - xaringan-themer.css
       - assets/css/custom.css
       - assets/css/tachyons.min.css
---

```{r xaringan-themer, include = FALSE}
options(htmltools.dir.version = FALSE)
library(icon)
library(xaringanthemer)
library(magrittr)
library(flextable)
solarized_light(
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "500", "400i"),
  code_font_google   = google_font("Droid Mono"),
  title_slide_background_color = "#002733",
  link_color = "#eb1455",
  text_bold_color = "#00589a",
  text_font_size = "25px",
  header_color = "#002733",
  padding = "1em 2em 1em 2em"
  
)
```

class: title-slide  

# Supporting Ecological Decision Making Using Feature-Selection and Variable Importance

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

### Patrick Schratz

<p style="margin-left:15px;">

<br>

ISEM 2019, October 4th 2019, Salzburg, Austria

<br><br>

`r fa_university()` <sup>1</sup> Department of Geography, GIScience group, University of Jena  <a href="https://www.geographie.uni-jena.de/Geoinformatik.html">`r fa_external_link_alt()`</a>

<br><br>

`r fa_home()` <a href="https://pjs-web.de">https://pjs-web.de</a> &emsp; 

`r fa_twitter()` <a href="https://twitter.com/pjs_228">@pjs_228</a> &emsp; 

`r fa_github()` <a href="https://github.com/pat-s">@pat-s</a> &emsp; 

`r fa_stack_exchange()` <a href="https://stackoverflow.com/users/4185785/pat-s">@pjs_228</a> &emsp;  <br>

`r fa_envelope()` <a href="patrick.schratz@uni-jena.de">patrick.schratz@uni-jena.de</a>&emsp;

`r fa_linkedin()` <a href="https://www.linkedin.com/in/patrick-schratz/">Patrick Schratz</a>&emsp;

</p>

---

# About me

.pull-left[

```{r, echo = FALSE}
knitr::include_graphics("https://pjs-web.de/authors/admin/avatar_hud979e34f5123fec6163cd5fb1cb16697_143342_250x250_fill_q75_box_center.jpg")
```

- M.Sc. Geoinformatics
- Researcher at University of **Jena**
- PhD Candidate

]

.pull-right[
- R developer at University of **Munich**
- Machine-learning in R ([mlr3](https://github.com/mlr-org/mlr3), [mlr](https://github.com/mlr-org/mlr))

```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/mlr-org/mlr/master/man/figures/logo_navbar.png")
```

### personal focus

- open-source enthusiast
- reproducible research
- high-performance computing
- AUR package maintainer

]

---

# Study Objectives

- Relate **defoliation** at trees to remote-sensing information 
  - raw band information
  - vegetation indices
  - normalized ration indices
  
- Evaluate various (machine/statistical)-learning methods on **predictive performance**

- Compare **dimension-reduction** methods (simple filters, ensemble filters, PCA)

- Analyze the effect of **combined feature sets**

- Which spectral regions/variables are **most important** for modeling defoliation?


---

class: inverse, center, middle

.big-inv[
Study Design
]

---

# Workflow

- Usage of the **R** programming language

- Completely **reproducible**

- "makefile" approach using the DSL of the .pkg[drake] package

- Model fitting/evaluation via the .pkg[mlr] package (machine learning framework in R)

- Research compendioum on [Github](https://github.com/pat-s/2019-feature-selection) and [Zenodo](https://zenodo.org/record/2633102)

---

# Defoliation

```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/pat-s/2019-feature-selection/master/code/98-paper/ieee/jpg/defol-grid-pres-1500px.jpg")
```

.large[
&emsp;&emsp; 10 % &emsp;&emsp;&emsp;&emsp;&emsp; 20 %  &emsp;&emsp;&emsp;&emsp;&ensp; 40%  &emsp;&emsp;&emsp;&emsp; 60-70%
]

---

<center><img src="https://raw.githubusercontent.com/pat-s/2019-feature-selection/master/code/98-paper/ieee/jpg/study-area.png" width = "800px"></center>

---

# Hyperspectral Data

.pull-left[
- 122 bands

- Band width: ~ 4 nm

- Wavelength range: 400 - 1000 nm

- Acquisition date: September 2016
]

.pull-right[
<center><img src="imgs/hs-img.png" width = "900px"></center>

<center>NDVI of Laukiz 1</center>
]
---

# Feature Set Creation

- Extraction of hyperspectral reflectances for each tree using a buffer of **two meters**

- Calculation of **90** vegetation indices

- Calculation of all possible "normalized ration indices" (NRI): **7470**

  $NRIᵢⱼ = \frac{bᵢ - bⱼ}{bᵢ + bⱼ}$
  

.pull-left[
- **HR** (hyperspectral reflectances)

- **VI** (vegetation indices)

- **NRI** (normalized ratio indices)
]

.pull-right[
- **HR** + **VI**

- **HR** + **NRI**

- **HR** + **VI** + **NRI**
]

---

# Benchmark Setup

.pull-left[

## Models

**Machine learning**

- Support Vector Machine (SVM)

- Random Forest (RF)

- Extreme Gradient Boosting (XGBoost)

**Statistical**

- L1 penalized regression (LASSO)
- L2 penalized regression (RIDGE)
]

.pull-right[

## Filters

- (PCA)

- Borda (Ensemble)

- Carscore

- Conditional mutual information criterion

- Information Gain

- Pearson correlation

- Relief
]

---

# Benchmark Setup

.pull-left[
## Hyperparameter optimization

- Model-based optimization (Bayesian)

  - Initial budget: 30
  - Sequential budget: 70

- Tuning of $p$ (number of variables) during parameter optimization

- Penalized methods: Optimization of penalization term $s$

]

.pull-right[

## Error Measure

Root Mean Square Error (RMSE)

## Resampling Strategy

 Nested 4-fold **spatial cross-validation** (plot-based)
]

---

class: inverse, center, middle

.big-inv[
Results
]

---

.flex[
.w-65.br0.shadow-1.mr4[

<center><img src="https://raw.githubusercontent.com/pat-s/2019-feature-selection/master/docs/figure/eval-performance.Rmd/performance-results-1.png" width = "710px"></center>

]

.w-30[

## First look

- ML > statistical?

- Not much difference between tasks

- Substantial differences between models

- RMSE ~ 34

<br>

`r icon::fa_arrow_right()` Too much information for a single presentation slide
]

]

---

.flex[
.w-50.mr4[

```{r, echo = FALSE}
readRDS(url("https://github.com/pat-s/2019-feature-selection/raw/master/code/98-paper/presentation/table-perf.rda"))[1:15,] %>% 
  flextable() %>% 
  fontsize(size = 25, part = "all") %>% 
  color(i = 1, color = "red")
```

<div style="height:40px;font-size:2px;">&nbsp;</div>

<center> Best 15 absolute scores </center>
]

.w-60[
```{r, echo = FALSE}
readRDS(url("https://github.com/pat-s/2019-feature-selection/raw/master/code/98-paper/presentation/table-best-learner-per-task.rda")) %>% 
  flextable::flextable() %>% 
    fontsize(size = 20, part = "all")
```

<div style="height:40px;font-size:2px;">&nbsp;</div>

<center> Best scores by learners across all tasks/filters </center>

]
]

---

# Intermediate Summary

`r icon::fa_arrow_right()` No practical difference between most learners regarding their overall performance

`r icon::fa_arrow_right()` Combination of feature sets does not substantially increase performance

`r icon::fa_arrow_right()` HS reflectance values result in almost similar performance than using indices

---

class: center, middle

.big-q[
What about the filter differences?
]

---

class: center, middle

.big-q[
Effect of filter methods in general (1/3)
]

---

class: center

<center><img src="https://raw.githubusercontent.com/pat-s/2019-feature-selection/master/docs/figure/eval-performance.Rmd/filter-effect-1.png" width = "620px"></center>

Model performances in RMSE when using no filter method compared to the best scoring filter method for each learner across all tasks

---

class: center, middle

.big-q[
Effect of ensemble filters (2/3)
]

---
class: center, middle

<center><img src="https://raw.githubusercontent.com/pat-s/2019-feature-selection/master/docs/figure/eval-performance.Rmd/filter-effect-ensemble-1.png" width = "620px"></center>

Model performances in RMSE when using the ensemble "Borda" filter method compared to the best scoring filter method for each learner across all tasks

---

class: center, middle

.big-q[
Effect of filtering by model (3/3)
]

---
class: center, middle

<center><img src="https://raw.githubusercontent.com/pat-s/2019-feature-selection/master/docs/figure/eval-performance.Rmd/filter-perf-all-1.png" width = "620px"></center>

Model performances in RMSE of all used filter methods and PCA for each learner across all tasks

---

# Summary

- Almost no effect of filter for SVM

- **Effect** of filters **differs** across tasks and models (no clear pattern)

- **Ensemble filters** showed **no improvement** in predictive performance compared to simple filters in this study

- **Statistical models** are able to reach **similar performance** as ML models in high-dim settings

- PCA is a valid alternative to filter methods

---

class: center, middle

<iframe src="https://giphy.com/embed/3o6gDSdED1B5wjC2Gc" width="480" height="378" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>

---

class: center, middle

<iframe src="https://giphy.com/embed/129NVCr1UfsGTS" width="480" height="269" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>

---

class: center, middle

.big-q[
Which features are most important for predicting **defoliation**?
]

---

